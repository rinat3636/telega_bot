"""
GC Worker –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–µ–∫—à–∏—Ö –∞—Å—Å–µ—Ç–æ–≤
–†–µ—à–µ–Ω–∏–µ F-304: —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ expires_at
"""
import os
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path

from database.models import db


logger = logging.getLogger(__name__)


class AssetGarbageCollector:
    """
    Garbage Collector –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–µ–∫—à–∏—Ö –∞—Å—Å–µ—Ç–æ–≤
    
    –§—É–Ω–∫—Ü–∏–∏:
    - –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ expires_at
    - –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö job –∑–∞–ø–∏—Å–µ–π
    - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """
    
    def __init__(self, assets_dir: str = "/home/ubuntu/rei_bot/assets"):
        self.assets_dir = Path(assets_dir)
        self.stats = {
            "files_deleted": 0,
            "bytes_freed": 0,
            "jobs_cleaned": 0,
            "errors": 0
        }
    
    async def run_gc_cycle(self):
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ü–∏–∫–ª GC
        
        1. –ù–∞–π—Ç–∏ –∏—Å—Ç–µ–∫—à–∏–µ job
        2. –£–¥–∞–ª–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        3. –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å job
        4. –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        logger.info("üóëÔ∏è Starting GC cycle...")
        
        try:
            # 1. –ù–∞–π—Ç–∏ –∏—Å—Ç–µ–∫—à–∏–µ job
            expired_jobs = await db.get_expired_jobs()
            
            if not expired_jobs:
                logger.info("‚úÖ No expired jobs found")
                return
            
            logger.info(f"Found {len(expired_jobs)} expired jobs")
            
            # 2. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—ã–π job
            for job in expired_jobs:
                await self._cleanup_job(job)
            
            # 3. –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            logger.info(
                f"üóëÔ∏è GC cycle completed: "
                f"files_deleted={self.stats['files_deleted']}, "
                f"bytes_freed={self.stats['bytes_freed']}, "
                f"jobs_cleaned={self.stats['jobs_cleaned']}, "
                f"errors={self.stats['errors']}"
            )
            
            # –°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats = {
                "files_deleted": 0,
                "bytes_freed": 0,
                "jobs_cleaned": 0,
                "errors": 0
            }
        
        except Exception as e:
            logger.error(f"‚ùå GC cycle failed: {e}", exc_info=True)
    
    async def _cleanup_job(self, job: dict):
        """
        –û—á–∏—Å—Ç–∏—Ç—å —Ñ–∞–π–ª—ã —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å job
        
        Args:
            job: –ó–∞–ø–∏—Å—å job –∏–∑ –ë–î
        """
        try:
            job_id = job['id']
            result_path = job.get('result_path')
            
            # –£–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if result_path and os.path.exists(result_path):
                file_size = os.path.getsize(result_path)
                os.remove(result_path)
                
                self.stats['files_deleted'] += 1
                self.stats['bytes_freed'] += file_size
                
                logger.info(f"üóëÔ∏è Deleted file: {result_path} ({file_size} bytes)")
            
            # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å job
            await db.update_job_status(job_id, 'expired', progress=100)
            self.stats['jobs_cleaned'] += 1
            
            logger.info(f"‚úÖ Cleaned job {job_id}")
        
        except Exception as e:
            logger.error(f"‚ùå Failed to cleanup job {job.get('id')}: {e}")
            self.stats['errors'] += 1
    
    async def cleanup_old_jobs(self, days: int = 30):
        """
        –û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ job
        
        Args:
            days: –£–¥–∞–ª–∏—Ç—å job —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            # –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ä—ã–µ job
            old_jobs = await db.get_jobs_before_date(cutoff_date)
            
            if not old_jobs:
                logger.info(f"‚úÖ No old jobs found (older than {days} days)")
                return
            
            logger.info(f"Found {len(old_jobs)} old jobs to cleanup")
            
            # –£–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª—ã –∏ –∑–∞–ø–∏—Å–∏
            for job in old_jobs:
                await self._cleanup_job(job)
                await db.delete_job(job['id'])
            
            logger.info(f"üóëÔ∏è Cleaned {len(old_jobs)} old jobs")
        
        except Exception as e:
            logger.error(f"‚ùå Failed to cleanup old jobs: {e}", exc_info=True)
    
    async def cleanup_orphaned_files(self):
        """
        –û—á–∏—Å—Ç–∏—Ç—å —Ñ–∞–π–ª—ã –±–µ–∑ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö job (orphaned files)
        """
        try:
            if not self.assets_dir.exists():
                logger.warning(f"Assets directory not found: {self.assets_dir}")
                return
            
            # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ job —Å —Ñ–∞–π–ª–∞–º–∏
            jobs_with_files = await db.get_jobs_with_files()
            valid_paths = {job['result_path'] for job in jobs_with_files if job.get('result_path')}
            
            # –ù–∞–π—Ç–∏ orphaned files
            orphaned_count = 0
            orphaned_size = 0
            
            for file_path in self.assets_dir.rglob("*"):
                if file_path.is_file():
                    file_path_str = str(file_path)
                    
                    if file_path_str not in valid_paths:
                        file_size = file_path.stat().st_size
                        file_path.unlink()
                        
                        orphaned_count += 1
                        orphaned_size += file_size
                        
                        logger.info(f"üóëÔ∏è Deleted orphaned file: {file_path_str}")
            
            if orphaned_count > 0:
                logger.info(
                    f"üóëÔ∏è Cleaned {orphaned_count} orphaned files "
                    f"({orphaned_size} bytes)"
                )
            else:
                logger.info("‚úÖ No orphaned files found")
        
        except Exception as e:
            logger.error(f"‚ùå Failed to cleanup orphaned files: {e}", exc_info=True)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä GC
gc = AssetGarbageCollector()


async def run_gc_worker(interval_hours: int = 6):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å GC worker –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ
    
    Args:
        interval_hours: –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏ GC (–≤ —á–∞—Å–∞—Ö)
    """
    logger.info(f"üóëÔ∏è GC Worker started (interval: {interval_hours} hours)")
    
    while True:
        try:
            # –ó–∞–ø—É—Å—Ç–∏—Ç—å GC —Ü–∏–∫–ª
            await gc.run_gc_cycle()
            
            # –û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ job (30 –¥–Ω–µ–π)
            await gc.cleanup_old_jobs(days=30)
            
            # –û—á–∏—Å—Ç–∏—Ç—å orphaned files
            await gc.cleanup_orphaned_files()
            
            # –ü–æ–¥–æ–∂–¥–∞—Ç—å –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞
            await asyncio.sleep(interval_hours * 3600)
        
        except Exception as e:
            logger.error(f"‚ùå GC Worker error: {e}", exc_info=True)
            # –ü–æ–¥–æ–∂–¥–∞—Ç—å 1 —á–∞—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            await asyncio.sleep(3600)


if __name__ == "__main__":
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å GC worker standalone
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    asyncio.run(run_gc_worker(interval_hours=6))
